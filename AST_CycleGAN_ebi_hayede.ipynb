{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AST_CycleGAN_ebi_hayede.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majidsh97/project-2/blob/main/AST_CycleGAN_ebi_hayede.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF3sNTU0T8cA"
      },
      "source": [
        "!git clone https://github.com/inzva/Audio-Style-Transfer.git\n",
        "!pip install -q tensorflow-io\n",
        "!pip install librosa==0.8\n",
        "!pip install tensorflow-gan\n",
        "!pip install -q -U tensorflow-addons\n",
        "#sss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B78UPGoERYRR"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow_io as tfio\n",
        "from tensorflow.keras import layers\n",
        "import librosa\n",
        "import time\n",
        "import tensorflow_gan as tfgan\n",
        "from scipy.signal import resample\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1enlwRRnEMoM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhqXlYVDSH9u"
      },
      "source": [
        "\n",
        "#newvalue = a * value + b. a = (max'-min')/(max-min) and b = max - a * max\n",
        "\n",
        "def tensor_to_audio(tensor,fs,N_FFT):\n",
        "  a = np.zeros_like(tensor)\n",
        "  a = np.exp(tf.transpose(tensor).numpy()) - 1\n",
        "  \n",
        "  # This code is supposed to do phase reconstruction\n",
        "  p = 2 * np.pi * np.random.random_sample(a.shape) - np.pi\n",
        "  for i in range(10):\n",
        "      S = a * np.exp(1j*p)\n",
        "      x = librosa.istft(S)\n",
        "      p = np.angle(librosa.stft(x, N_FFT))\n",
        "  return x\n",
        "  OUTPUT_FILENAME =   '1.wav' \n",
        "  librosa.write_wav(OUTPUT_FILENAME, x, fs)\n",
        "\n",
        "def gram(inputs):\n",
        "  #batch nist age batch mikhay bayad ye b be harf ha va ye 1 be inputs.shape azafe beshe\n",
        "  gram=0\n",
        "  gram = tf.matmul(tf.transpose(inputs),inputs)/(inputs.shape[0]*inputs.shape[1])\n",
        "    \n",
        "  \"\"\"  if len(inputs.shape)==4:\n",
        "    ij=tf.cast(inputs.shape[1]*inputs.shape[2],'float32')\n",
        "    gram=tf.linalg.einsum('bijk,bijd->bkd',inputs,inputs)/ij\n",
        "  else:\n",
        "   if len(inputs.shape)==3:\n",
        "    i=tf.cast(inputs.shape[1],'float32')\n",
        "    gram=tf.linalg.einsum('bjk,bjd->bkd',inputs,inputs)/i\"\"\"\n",
        "\n",
        "  return gram\n",
        "\n",
        "def abs_angle_to_complex(abs,ang):\n",
        "  ang=tf.complex(0.0,ang)\n",
        "  ang=tf.math.exp(ang)\n",
        "  z=tf.complex(abs,0.0)*ang\n",
        "  return z\n",
        "\n",
        "def t2a(tensor, frame_length, frame_step, fft_length):\n",
        "  a=tf.complex(tensor,0.0)\n",
        "  a=tf.cast(a,tf.complex64)\n",
        "  a = tf.math.exp(a) - 1\n",
        "  p = 2 * np.pi* tf.random.uniform(a.shape,dtype=tf.float32) - np.pi\n",
        "  p=tf.complex(0.0,p)\n",
        "  for i in range(20):\n",
        "    S = a * tf.math.exp(p)\n",
        "    x=tf.signal.inverse_stft(S, frame_length, frame_step, fft_length)\n",
        "    stft=tf.signal.stft(x,frame_length, frame_step,fft_length)\n",
        "    p = tf.math.angle(stft)\n",
        "    p = tf.complex(0.0,p)\n",
        "  return x\n",
        "\n",
        "def imshow(img):\n",
        "  plt.imshow(img+127.5/255.0)\n",
        "\n",
        "def linear_scale(x,max_to=1,min_to=0):\n",
        "  max=x.max()\n",
        "  min=x.min()\n",
        "  a = (max_to-min_to)/(max-min)\n",
        "  b = max - a * max\n",
        "  newvalue = a * x + b  \n",
        "  return newvalue\n",
        "\n",
        "\n",
        "def display(x,fs):\n",
        "  IPython.display.display(IPython.display.Audio(x,rate=fs))\n",
        "\n",
        "def invspectogram(spect,angle,frame_length,frame_step,fft_length):\n",
        "  spect = abs_angle_to_complex(tf.math.expm1(spect),angle)\n",
        "  y_invstft=tf.signal.inverse_stft(spect,frame_length,frame_step,fft_length)\n",
        "  return y_invstft\n",
        "\n",
        "def spectrogram(x,frame_length , frame_step,fft_length ):\n",
        "  y=tf.signal.stft(x,frame_length,frame_step,fft_length)\n",
        "  content_angle=tf.math.angle(y)\n",
        "  content_spect=tf.math.log1p(tf.abs(y))\n",
        "  return [content_spect , content_angle]\n",
        "\n",
        "def window_stack(a, stepsize, width):\n",
        "    return tf.stack( [a[i:1+i-width or None:stepsize]  for i in range(0,width) ],-1)\n",
        "\n",
        "class ClipConstraint(tf.keras.constraints.Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        " \n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn tf.keras.backend.clip(weights, -self.clip_value, self.clip_value)\n",
        " \n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVTvMsYrnrL4"
      },
      "source": [
        "\"\"\"a=tfio.audio.AudioIOTensor(FILE)\n",
        "fs=a.rate.numpy()\n",
        "a=a.to_tensor()[:,0]\n",
        "a=tf.cast(a,tf.float32).numpy()\n",
        "a = a / 32768.0\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzxb0Y7kImA3"
      },
      "source": [
        "\n",
        "def get_sliced_spectrom(s,p):\n",
        "  #x=tf.roll(x,tf.random.uniform((),0,int(n/2),dtype=tf.int32),0)\n",
        "  #x = window_stack(x,int(fs_content),fs_content)[...,None]\n",
        "\n",
        "  t = int(s.shape[0]/s.shape[1])*s.shape[1]\n",
        "  s_slice = np.reshape(s[:t,:],(-1,s.shape[1],s.shape[1]))\n",
        "  p_slice = np.reshape(p[:t,:],(-1,s.shape[1],s.shape[1]))\n",
        "  l2=tf.reduce_sum(tf.square(s_slice),[1,2])\n",
        "  condition=l2>1e4\n",
        "  s_slice = s_slice[condition,...,None]\n",
        "  p_slice = p_slice[condition,...,None]\n",
        "\n",
        "  return tf.concat( [s_slice,p_slice ] ,-1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgRpPDh3F3iX"
      },
      "source": [
        "frame_length=1024\n",
        "frame_step=256\n",
        "fft_length = None\n",
        "\"\"\"\n",
        "singer = 'ebi'\n",
        "DIR='/content/drive/My Drive/music/'+singer+'/'\n",
        "musiclist=os.listdir(DIR)\n",
        "data_list=[]\n",
        "for l in musiclist:\n",
        "  print(l)\n",
        "  \n",
        "  FILE=DIR+l+'/vocals.wav'\n",
        "  a,fs=librosa.load(FILE)\n",
        "\n",
        "  s,p = spectrogram(a,frame_length,frame_step,fft_length)\n",
        "  s=s[:,:s.shape[1]-1]\n",
        "  p=p[:,:p.shape[1]-1]\n",
        "  data_list.append(get_sliced_spectrom(s,p))\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9TgS5vtH7i4"
      },
      "source": [
        "hayede_np = np.load('/content/drive/My Drive/music/hayede.npy','r')\n",
        "ebi_np = np.load('/content/drive/My Drive/music/ebi.npy','r')\n",
        "\n",
        "BATCH_SIZE=1\n",
        "hayede = tf.data.Dataset.from_tensor_slices(hayede_np).shuffle(100).batch(BATCH_SIZE,True).prefetch(2)\n",
        "ebi = tf.data.Dataset.from_tensor_slices(ebi_np).shuffle(100).batch(BATCH_SIZE,True).prefetch(2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izPK-DMzKJIg"
      },
      "source": [
        "stride=1\n",
        "dilation=1\n",
        "init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
        "const = tf.keras.constraints.MaxNorm() #ClipConstraint(0.01)\n",
        "\n",
        "class WeightedSum(tf.keras.layers.Layer):\n",
        "  def __init__(self,name):\n",
        "    super(WeightedSum,self).__init__()\n",
        "    self.alpha = tf.Variable( tf.abs(init((1,))), trainable=True ,  dtype=tf.float32 , name=name )\n",
        "\n",
        "  def call(self,inp):\n",
        "    #return (1-self.alpha) * inp[0] + self.alpha * inp[1]\n",
        "    return (1-self.alpha) *inp[0] +  self.alpha * inp[1] #+ tf.random.normal(inp[1].shape[1:],0.02)\n",
        "\n",
        "\n",
        "class discBlock(layers.Layer):\n",
        "  def __init__(self,f):\n",
        "    super(discBlock,self).__init__()\n",
        "    #-------------------------------\n",
        "    self.c1 = layers.Conv2D(f,5,stride,dilation_rate=dilation,padding='SAME',use_bias=False, kernel_initializer=init , kernel_constraint=const )\n",
        "    self.c2 = layers.Conv2D(f,5,stride,dilation_rate=dilation,padding='SAME',use_bias=False, kernel_initializer=init , kernel_constraint=const )\n",
        "    #self.bn = layers.BatchNormalization()\n",
        "    #self.innorm = tfa.layers.normalizations.InstanceNormalization()\n",
        "\n",
        "    self.lr = layers.LeakyReLU(0.2)\n",
        "    #self.drop1=layers.Dropout(0.1)\n",
        "    #-------------------------------\n",
        "    self.avg = layers.AveragePooling2D()\n",
        "    #-------------------------------\n",
        "\n",
        "    \n",
        "  def call(self,inp):\n",
        "    x = inp\n",
        "    x = self.c1(inp)\n",
        "    x = self.c2(x)\n",
        "    #x = self.bn(x)\n",
        "    #x = self.innorm(x)\n",
        "    x = self.lr(x)\n",
        "    #x = self.drop1(x)\n",
        "    x = self.avg(x)\n",
        "    return x\n",
        "  \n",
        "  def plot(self):\n",
        "    _inp = layers.Input((32,32,3))\n",
        "    o=self.call(_inp)\n",
        "    m=tf.keras.Model(_inp,o)\n",
        "    img = tf.keras.utils.plot_model(m,'discBlock.png',True,dpi=64)\n",
        "    print(\"discBlock\")\n",
        "    IPython.display.display(img)\n",
        "    print('------------------------------')\n",
        "\n",
        "\n",
        "class genBlock(layers.Layer):\n",
        "  def __init__(self,f,use_last_layer):\n",
        "    super(genBlock,self).__init__()\n",
        "    #-------------------------------\n",
        "    self.up1 = layers.UpSampling2D()\n",
        "  #-----------------------------------------\n",
        "  \n",
        "    self.c1 = layers.Conv2DTranspose(f,5,stride,dilation_rate=dilation,padding='SAME',use_bias=False, kernel_initializer=init , kernel_constraint=const )\n",
        "    self.c2 = layers.Conv2DTranspose(f,5,stride,dilation_rate=dilation,padding='SAME',use_bias=False, kernel_initializer=init , kernel_constraint=const )\n",
        "    #self.bn = layers.BatchNormalization()\n",
        "    self.innorm = tfa.layers.normalizations.InstanceNormalization()\n",
        "    self.lr = layers.LeakyReLU(0.2)\n",
        "    self.use_last_layer = use_last_layer\n",
        "    if self.use_last_layer == True:\n",
        "      self.last_layer = layers.Conv2DTranspose(1,5,padding='SAME',use_bias=True, kernel_initializer=init , kernel_constraint=const )\n",
        "\n",
        "    #-------------------------------\n",
        "    \n",
        "  def call(self,inp):\n",
        "    x = inp\n",
        "    x = self.up1(x)\n",
        "    x = self.c1(x)\n",
        "    x = self.c2(x)\n",
        "    #x = self.bn(x)\n",
        "    x = self.innorm(x)\n",
        "    x = self.lr(x)\n",
        "    if self.use_last_layer == True:\n",
        "      x = self.last_layer(x)\n",
        "\n",
        "    return x\n",
        "    \n",
        "  def plot(self):\n",
        "    _inp = layers.Input((32,32,3))\n",
        "    o=self.call(_inp)\n",
        "    m=tf.keras.Model(_inp,o)\n",
        "    img = tf.keras.utils.plot_model(m,'genBlock.png',True,dpi=64)\n",
        "    print(\"genBlock\")\n",
        "    IPython.display.display(img)\n",
        "    print('------------------------------')\n",
        "  \n",
        "    \n",
        "\n",
        "def udown(inp,f):\n",
        "  x = inp\n",
        "  f = f * 2\n",
        "  #x = layers.Conv2D(f,5,stride,dilation_rate=dilation,padding='SAME',use_bias=False, kernel_initializer=init , kernel_constraint=const )(x)\n",
        "  #------------------------ bala bayad add she -----------------------------------\n",
        "  x = discBlock(f)(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def uup(inp,f,use_last_layer):\n",
        "  x=inp\n",
        "  f=int(f/2)\n",
        "  x = genBlock(f,use_last_layer)(x)\n",
        "  #------------------------ paiin bayad add she -----------------------------------\n",
        "  #x = layers.Conv2DTranspose(f,5,stride,dilation_rate=dilation,padding='SAME',use_bias=False, kernel_initializer=init , kernel_constraint=const )(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def get_discriminator(inp_shape,num,f):\n",
        "  inp = layers.Input(inp_shape,dtype=tf.float32)\n",
        "  x=inp\n",
        "  layer_list=[]\n",
        "  for i in range(num):\n",
        "    x=udown(x,f)\n",
        "    f=x.shape[-1]\n",
        "    layer_list.append(x)\n",
        "\n",
        "  #x = layers.GlobalAveragePooling2D()(x)\n",
        "  #x = layers.Conv2D(x.shape[-1],4,use_bias=False,kernel_initializer=init)(x)\n",
        "  #x = layers.Flatten()(x)\n",
        "  #x = layers.Dense(1,use_bias=True,kernel_initializer=init,kernel_constraint=const)(x)\n",
        "  return tf.keras.models.Model(inp,x)\n",
        "\n",
        "def get_generator(input_shape,num,f,latent_dim,is_latent=True):\n",
        "  inp = layers.Input(input_shape,dtype=tf.float32)\n",
        "  x = inp\n",
        "  use_last_layer = False\n",
        "\n",
        "  if is_latent == True:\n",
        "    inp = layers.Input((latent_dim,),dtype=tf.float32)\n",
        "    x = layers.Dense(input_shape[0]*input_shape[1]*input_shape[2],kernel_constraint=const)(x)\n",
        "    x = layers.Reshape(input_shape)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "  \n",
        "  \n",
        "  for i in range(num):\n",
        "    if i == num-1:\n",
        "      use_last_layer=True\n",
        "\n",
        "    x=uup(x,f,use_last_layer)\n",
        "    f=x.shape[-1]\n",
        "\n",
        "\n",
        "  return tf.keras.models.Model(inp,x)\n",
        "\n",
        "\n",
        "gb=genBlock(3,True)\n",
        "gb.plot()\n",
        "db=discBlock(3)\n",
        "db.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyR5kC6oCNOE"
      },
      "source": [
        "\"\"\"def get_filters(f_size):\n",
        "  filters = [f_size*2**i for i in range(0,6)]\n",
        "  return filters\n",
        "\"\"\"\n",
        "#discriminator.trainable = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arorfOWmcmK_"
      },
      "source": [
        "class UentClass():\n",
        "  def __init__(self,INPUT_SHAPE,NUM,filter_init_size,latent_dim):\n",
        "\n",
        "    self.discriminator = get_discriminator(INPUT_SHAPE,NUM,filter_init_size)\n",
        "    self.generator = get_generator(self.discriminator.output_shape[1:],NUM,self.discriminator.output_shape[-1],latent_dim,False)\n",
        "\n",
        "    #self.plot()\n",
        "\n",
        "    #_x=_y=int(INPUT_SHAPE[0]/2**num)\n",
        "    #_z=2**(num+1)\n",
        "    #generator = get_generator((_h,_w,_z),num,_z)\n",
        "\n",
        "  def plot(self):\n",
        "    for m in [self.discriminator , self.generator]:\n",
        "      print(len(m.layers))\n",
        "      m.summary()\n",
        "      img = tf.keras.utils.plot_model(m,dpi=64,show_shapes=True)\n",
        "      IPython.display.display(img)\n",
        "  def get_layers(self):\n",
        "\n",
        "    return zip(self.discriminator.layers[1:] , reversed(self.generator.layers[5:]))\n",
        "    #print(d.name , d.input_shape,g.name ,g.output_shape)\n",
        "\n",
        "  def set_trainability(self,flag):\n",
        "    for d,g in self.get_layers():\n",
        "      d.trainable=flag\n",
        "      g.trainable=flag \n",
        "\n",
        "  def get_unet(self,g_in=0,d_out=False ):\n",
        "\n",
        "    x=self.discriminator.input\n",
        "    q=[]\n",
        "    #q.append(x)\n",
        "    for i in range(len(self.discriminator.layers)):\n",
        "      l = self.discriminator.layers[i]\n",
        "      if isinstance(l,layers.InputLayer)==True:\n",
        "        continue\n",
        "\n",
        "      x=l(x)\n",
        "      q.append(x)\n",
        "\n",
        "\n",
        "    q.reverse()\n",
        "    print(q)\n",
        "    print(len(q))\n",
        "    #x = layers.UpSampling2D()(x)\n",
        "    #x = layers.Concatenate()([x,x])\n",
        "    #--------------------------------------------------\n",
        "    #x = layers.Flatten()(x)\n",
        "    #x = layers.Dense(x.shape[1])(x)\n",
        "    #x = layers.Reshape(generator.input_shape[1:])(x)\n",
        "    #--------------------------------------------------\n",
        "    disc_output = x\n",
        "    if d_out == True:\n",
        "      disc_output = layers.Flatten()(disc_output)\n",
        "      disc_output = layers.Dense(1)(disc_output)\n",
        "    \n",
        "    #x = layers.Flatten()(x)\n",
        "    if g_in==1:\n",
        "      x = self.generator.input\n",
        "    if g_in == 2:\n",
        "      x = WeightedSum('alpha_middle')([x,self.generator.input])\n",
        "\n",
        "\n",
        "    for i in range(len(self.generator.layers)):\n",
        "      l = self.generator.layers[i]\n",
        "      if isinstance(l,layers.InputLayer)==True:\n",
        "        continue \n",
        "      x=l(x)\n",
        "      if i < len(q):\n",
        "        print(x,q[i])\n",
        "        #x = WeightedSum(\"alpha_\"+str(i))([q[i] , x])\n",
        "        x = layers.Add()([q[i],x])\n",
        "      \n",
        "\n",
        "    #x  = layers.Conv2DTranspose(1,5,padding='SAME')(x)\n",
        "    #x = tf.nn.tanh(x)\n",
        "    outputs=[]\n",
        "    inputs=[self.discriminator.input]\n",
        "    if d_out==True:\n",
        "      outputs.append(disc_output)\n",
        "\n",
        "    outputs.append(x)\n",
        "\n",
        "    if g_in==1 or g_in == 2:\n",
        "      inputs.append(self.generator.input)\n",
        "\n",
        "    return tf.keras.Model(inputs,outputs)\n",
        "\n",
        "\"\"\"unetclass=UentClass((512,512,1),4,16,256)\n",
        "unetclass.plot()\n",
        "layer_zipped =unetclass.get_layers()\n",
        "for d,g in layer_zipped:\n",
        "  print(d.name , d.input_shape,g.name ,g.output_shape)\n",
        "\n",
        "\n",
        "unet= unetclass.get_unet(0,False)\n",
        "unet.summary()\n",
        "tf.keras.utils.plot_model(unet,dpi=64,show_shapes=True)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9Cs2p9PEf29"
      },
      "source": [
        "def get_alphas(_model):\n",
        "  alphas=[]\n",
        "  for v in _model.non_trainable_variables:\n",
        "    if v.name[:5]=='alpha':\n",
        "      alphas.append(v)\n",
        "  return alphas\n",
        "\n",
        "def set_alphas(alphas,new_values):\n",
        "  for a,nv in zip(alphas,new_values):\n",
        "    a.assign(nv)\n",
        "\n",
        "#alphas= get_alphas(unet)\n",
        "#print(alphas)\n",
        "#new_values = tf.ones_like(alphas) * 0.2\n",
        "#set_alphas(alphas , new_values)\n",
        "#print(alphas)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJuHI-LTZYNZ"
      },
      "source": [
        "filter_init_size=7\n",
        "latent_dim = 256\n",
        "NUM = 5\n",
        "generator_g_class=UentClass((512,512,1),NUM,filter_init_size,latent_dim)\n",
        "generator_g= generator_g_class.get_unet(0,False)\n",
        "\n",
        "generator_f_class=UentClass((512,512,1),NUM,filter_init_size,latent_dim)\n",
        "generator_f= generator_f_class.get_unet(0,False)\n",
        "\n",
        "generator_g.summary()\n",
        "tf.keras.utils.plot_model(generator_g,dpi=64,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtMc4JnufCEr"
      },
      "source": [
        "#discriminator_extended = unetclass.discriminator\n",
        "discriminator_x = get_discriminator((512,512,1),NUM,filter_init_size)\n",
        "x = discriminator_x.output\n",
        "#x = discBlock(x.shape[-1])(x)\n",
        "x = layers.Flatten()(x)\n",
        "#x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(1,kernel_constraint=const)(x)\n",
        "discriminator_x = tf.keras.Model(discriminator_x.input , x )\n",
        "\n",
        "\n",
        "discriminator_y = get_discriminator((512,512,1),NUM,filter_init_size)\n",
        "x = discriminator_y.output\n",
        "#x = discBlock(x.shape[-1])(x)\n",
        "x = layers.Flatten()(x)\n",
        "#x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(1,kernel_constraint=const)(x)\n",
        "discriminator_y = tf.keras.Model(discriminator_y.input , x )\n",
        "discriminator_y.summary()\n",
        "tf.keras.utils.plot_model(discriminator_y,dpi=64,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2iKdKEHBcj4"
      },
      "source": [
        "\n",
        "NUM_SAMPLE=5\n",
        "x = next(iter(ebi))\n",
        "x_spect_const= x[...,0:1]\n",
        "x_angle_const= x[...,1:2]\n",
        "y = next(iter(hayede))\n",
        "y_spect_const= y[...,0:1]\n",
        "y_angle_const= y[...,1:2]\n",
        "\n",
        "\"\"\"x_spect_const= ebi_spect[NUM_SAMPLE][None,...,None]\n",
        "x_angle_const= ebi_angle[NUM_SAMPLE][None,...,None]\n",
        "\n",
        "y_spect_const=hayede_spect[NUM_SAMPLE][None,...,None]\n",
        "y_angle_const=hayede_angle[NUM_SAMPLE][None,...,None]\"\"\"\n",
        "\n",
        "print(x_angle_const.shape)\n",
        "\n",
        "def result(x_spect,x_angle,y_spect,y_angle):\n",
        "  y_generated= generator_g(x_spect,training=False)\n",
        "  x_generated=generator_f(y_spect,training=False)\n",
        "  #y_generated = tf.nn.relu((y_generated+0.5)*5.0)\n",
        "  #x_generated = tf.nn.relu((x_generated+0.5)*5.0)\n",
        "\n",
        "  plt.figure(figsize=(8,8))\n",
        "  i=0\n",
        "  for x in [(x_spect, x_angle) , (x_generated,x_angle), (y_spect,y_angle) , (y_generated,y_angle)]:\n",
        "    spect = x[0][0,:,:,0]\n",
        "    angle = x[1][0,:,:,0]\n",
        "\n",
        "    spect = tf.nn.relu((spect+1)*2.5)\n",
        "    angle = angle * np.pi\n",
        "    print(np.max(spect))\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.imshow(spect)\n",
        "    plt.axis(False)\n",
        "    a = t2a(spect,frame_length, frame_step, 1023)\n",
        "    #a = invspectogram(spect,angle,frame_length,frame_step,1023)\n",
        "    display(a,22050)\n",
        "    i+=1\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "result(x_spect_const,x_angle_const,y_spect_const,y_angle_const)\n",
        "\n",
        "LAMBDA = 10\n",
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "def discriminator_loss(real, generated):\n",
        "  real_loss = loss_obj(tf.ones_like(real), real)\n",
        "\n",
        "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss * 0.5\n",
        "\n",
        "def generator_loss(generated):\n",
        "  return loss_obj(tf.ones_like(generated), generated)\n",
        "\n",
        "def identity_loss(real_image, same_image):\n",
        "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "  return LAMBDA * 0.5 * loss\n",
        "\n",
        "def calc_cycle_loss(real_image, cycled_image):\n",
        "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "  \n",
        "  return LAMBDA * loss1\n",
        "\n",
        "LR =1e-4\n",
        "generator_g_optimizer = tf.keras.optimizers.Adam(LR, beta_1=0.0)\n",
        "generator_f_optimizer = tf.keras.optimizers.Adam(LR, beta_1=0.0)\n",
        "\n",
        "discriminator_x_optimizer = tf.keras.optimizers.Adam(LR, beta_1=0.0)\n",
        "discriminator_y_optimizer = tf.keras.optimizers.Adam(LR, beta_1=0.0)\n",
        "\n",
        "@tf.function\n",
        "def train_step(real_x, real_y):\n",
        "  # persistent is set to True because the tape is used more than\n",
        "  # once to calculate the gradients.\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Generator G translates X -> Y\n",
        "    # Generator F translates Y -> X.\n",
        "    \n",
        "    fake_y = generator_g(real_x, training=True)\n",
        "    cycled_x = generator_f(fake_y, training=True)\n",
        "\n",
        "    fake_x = generator_f(real_y, training=True)\n",
        "    cycled_y = generator_g(fake_x, training=True)\n",
        "\n",
        "    # same_x and same_y are used for identity loss.\n",
        "    same_x = generator_f(real_x, training=True)\n",
        "    same_y = generator_g(real_y, training=True)\n",
        "\n",
        "    disc_real_x = discriminator_x(real_x, training=True)\n",
        "    disc_real_y = discriminator_y(real_y, training=True)\n",
        "\n",
        "    disc_fake_x = discriminator_x(fake_x, training=True)\n",
        "    disc_fake_y = discriminator_y(fake_y, training=True)\n",
        "\n",
        "    # calculate the loss\n",
        "    gen_g_loss = generator_loss(disc_fake_y)\n",
        "    gen_f_loss = generator_loss(disc_fake_x)\n",
        "    \n",
        "    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
        "    \n",
        "    # Total generator loss = adversarial loss + cycle loss\n",
        "    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
        "    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
        "\n",
        "    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
        "    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
        "  \n",
        "  # Calculate the gradients for generator and discriminator\n",
        "  generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
        "                                        generator_g.trainable_variables)\n",
        "  generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
        "                                        generator_f.trainable_variables)\n",
        "  \n",
        "  discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
        "                                            discriminator_x.trainable_variables)\n",
        "  discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
        "                                            discriminator_y.trainable_variables)\n",
        "  \n",
        "  # Apply the gradients to the optimizer\n",
        "  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
        "                                            generator_g.trainable_variables))\n",
        "\n",
        "  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
        "                                            generator_f.trainable_variables))\n",
        "  \n",
        "  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
        "                                                discriminator_x.trainable_variables))\n",
        "  \n",
        "  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
        "                                                discriminator_y.trainable_variables))\n",
        "  return [gen_g_loss , gen_f_loss , disc_x_loss , disc_y_loss]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpTabEOzpwx-"
      },
      "source": [
        "history=[] \n",
        "#alphas= get_alphas(generator)\n",
        "EPOCHS=15\n",
        "for e in range(0,EPOCHS):\n",
        "  start = time.time()   \n",
        "  \n",
        "  for x,y in zip(ebi,hayede):\n",
        "    h = train_step(x[...,0:1],y[...,0:1])\n",
        "  \n",
        "  if e%2==1:\n",
        "    hayede = tf.data.Dataset.from_tensor_slices(hayede_np).shuffle(100).batch(BATCH_SIZE,True).prefetch(2)\n",
        "    ebi = tf.data.Dataset.from_tensor_slices(ebi_np).shuffle(100).batch(BATCH_SIZE,True).prefetch(2)\n",
        "\n",
        "  print ('Time for epoch {} is {} sec gen_g_loss {} , gen_f_loss {} , disc_x_loss {} , disc_y_loss {}'.format(e+1, time.time()-start,h[0],h[1] ,h[2],h[3]))\n",
        "  history.append(h)\n",
        "  result(x_spect_const,x_angle_const,y_spect_const,y_angle_const)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0ngCIJDmE4l"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "y = np.array(history)\n",
        "plt.plot(y,'.-')\n",
        "plt.ylim()#[-1,5])\n",
        "plt.legend(['loss_g' , 'loss_f' , 'disc_real_loss_x' , 'disc_fake_loss_y' ])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5AWG8lC6A6O"
      },
      "source": [
        "\n",
        "\"\"\"x = next(iter(ebi))\n",
        "x_spect_const= x[...,0:1]\n",
        "x_angle_const= x[...,1:2]\n",
        "y = next(iter(hayede))\n",
        "y_spect_const= y[...,0:1]\n",
        "y_angle_const= y[...,1:2]\n",
        "\"\"\"\n",
        "\"\"\"x_spect_const= ebi_spect[NUM_SAMPLE][None,...,None]\n",
        "x_angle_const= ebi_angle[NUM_SAMPLE][None,...,None]\n",
        "\n",
        "y_spect_const=hayede_spect[NUM_SAMPLE][None,...,None]\n",
        "y_angle_const=hayede_angle[NUM_SAMPLE][None,...,None]\"\"\"\n",
        "\n",
        "print(x_angle_const.shape)\n",
        "\n",
        "def result(x_spect,x_angle,y_spect,y_angle):\n",
        "  y_generated= generator_g(x_spect,training=False)\n",
        "  x_generated=generator_f(y_spect,training=False)\n",
        "  #y_generated = tf.nn.relu((y_generated+0.5)*5.0)\n",
        "  #x_generated = tf.nn.relu((x_generated+0.5)*5.0)\n",
        "\n",
        "  plt.figure(figsize=(8,8))\n",
        "  i=0\n",
        "  for x in [(x_spect, x_angle) , (x_generated,x_angle), (y_spect,y_angle) , (y_generated,y_angle)]:\n",
        "    spect = x[0][0,:,:,0]\n",
        "    angle = x[1][0,:,:,0]\n",
        "\n",
        "    spect = tf.nn.relu((spect+1)*2.5)\n",
        "    angle = angle * np.pi\n",
        "    print(np.max(spect))\n",
        "    plt.subplot(2,2,i+1)\n",
        "    plt.imshow(spect)\n",
        "    plt.axis(False)\n",
        "    a = t2a(spect,frame_length, frame_step, 1023)\n",
        "    #a = invspectogram(spect,angle,frame_length,frame_step,1023)\n",
        "    display(a,22050)\n",
        "    i+=1\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "result(x_spect_const,y_angle_const,y_spect_const,x_angle_const)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}